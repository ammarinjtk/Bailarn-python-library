{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bailarn.ner import NamedEntityRecognizer,constant\n",
    "from bailarn.utils import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read raw text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokentizer model\n",
    "\n",
    "from bailarn.tokenizer import constant as tokenizer_constant\n",
    "from bailarn.tokenizer.tokenizer import Tokenizer\n",
    "\n",
    "# Create index for character and tag\n",
    "char_index = utils.build_tag_index(tokenizer_constant.CHARACTER_LIST, tokenizer_constant.CHAR_START_INDEX)\n",
    "tag_index = utils.build_tag_index(tokenizer_constant.TAG_LIST, tokenizer_constant.TAG_START_INDEX)\n",
    "\n",
    "tokenizer_model = Tokenizer(char_index, tag_index)\n",
    "\n",
    "def tokenize_func(sentence):\n",
    "    return tokenizer_model.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = utils.TextCollection(corpus_directory=\"./data/sample_BEST2010_I2R/\", tokenize_function=tokenize_func)\n",
    "print(\"Corpus size : {}\\n\".format(texts.count))\n",
    "print(\"Example corpus text : {}\\n\".format(texts.get_content(0)[0:500]))\n",
    "print(\"Tokenized content : {} \\n\".format(texts.get_token_list(0)[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read pre-tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "texts = utils.TextCollection(corpus_directory=\"./data/sample_BEST2010_I2R\", word_delimiter=\"|\",tag_delimiter=\"/\")\n",
    "print(\"Corpus size : {}\\n\".format(texts.count))\n",
    "print(\"Example corpus text : {}\\n\".format(texts.get_content(0)[0:500]))\n",
    "print(\"Tokenized content : {} \\n\".format(texts.get_token_list(0)[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word and tag indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Word Indexer from vocabs\n",
    "\n",
    "word_index = utils.build_word_index(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved word index\n",
    "\n",
    "import json\n",
    "\n",
    "with open('./bailarn/ner/ner_word_index.json', 'r') as f:\n",
    "    word_index = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./bailarn/ner/ner_word_index.json\", \"w\") as write_file:\n",
    "#     json.dump(word_index, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(word_index.items(), key=lambda x:x[1], reverse=False)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_index = utils.build_tag_index(constant.TAG_LIST, start_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform text into input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = utils.build_input(text_collection=texts, \n",
    "                       word_index=word_index, \n",
    "                       tag_index=tag_index, \n",
    "                       sequence_length=constant.SEQUENCE_LENGTH, \n",
    "                       target=\"ner\", \n",
    "                       for_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.x[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.readable_x[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.readable_y[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use default NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- predict on pantip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can manually add text in TextCollection\n",
    "\n",
    "sample_text = \"\"\"แจ้งเตือนดีลลวงโลกของNasambkและshopeeต้องอ่านเพื่อไม่เป็นเหยื่ออัพเดตทางshopeeติดต่อมาครับส่วนระบบ\n",
    "ใครจะถูกผิดผมไม่ทราบได้สิ่งที่ควรทำคือปรับปรุงแก้ไขไม่ว่าจะทางร้านหรือทางShopeeโดยทางshopeeออกโค้ดส่วนลดให้ใหม่\n",
    "และผมได้เครื่องเรียบร้อยขอบคุณการแก้ไขของshopeeส่วนร้านก็อยากให้ปรับปรุงเพราะผมยังคงกังขาทุกอย่างมันดูแปลกๆแจ้งเตือนดีล\n",
    "ของร้านNASAmbkในshopeeตามที่ผมคาดการณ์ไว้ยื้อการส่งของจนหมดเวลาระบบshopeeจะยกเลิกออเดอร์อัตโนมัติผมกดขยายเวลาจัดส่ง\n",
    "ก็ไม่ช่วยอะไรเพราะทางร้านต้นทางไม่กดยอมรับผมได้สั่งซื้อMate00proที่เป็นdealoftheday00,000บาทในวันที่00กพ.\n",
    "ระบบแจ้งให้ส่งของภายในวันที่00กพ.ปรากฎว่าร้านยื้อไม่ส่งของอ้างสารพัดจนหมดเวลาส่งระบบshopeeจึงทำการยกเลิกออเดอร์อัตโนมัติ\n",
    "ไม่ได้ของในราคานั้นมันคือขบวนการหลอกลวงต้มตุ๋นนั่นเองดูรายละเอียดในภาพนะครับช่วงแรกคุยกับร้านอีกช่วงคุยกับshopee\"\"\"\n",
    "\n",
    "texts = utils.TextCollection(\"/\", tokenize_function=tokenize_func)\n",
    "texts.add_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = len(texts.get_token_list(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = utils.build_input(text_collection=texts,\n",
    "                        word_index=word_index,\n",
    "                        tag_index=tag_index,\n",
    "                        sequence_length=constant.SEQUENCE_LENGTH, # padding size\n",
    "                        target='ner', for_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NamedEntityRecognizer(tag_index=tag_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.predict(vs.x, decode_tag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(vs.x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
